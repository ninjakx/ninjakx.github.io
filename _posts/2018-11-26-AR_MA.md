---
layout: post
title: "Introduction to AR and MA model"
featured-img: nn
categories: [MACHINE LEARNING]
---

# Auto Regressive(AR) Model

This model uses observations from some no of lagged observation i.e it takes observations from previous time steps as input to a regression equation to predict the value at the next time step.Or in other word, It predicts future behaviour based on past behaviour. 

> It’s used when there is some correlation between values in a time series and the values that precede and succeed them.It is an example of stochastic process, which have degrees of uncertainty or randomness in it. The randomness means that you might be able to predict future trends pretty well with past data, but you’re never going to get 100 percent accuracy.

**Source:** https://www.statisticshowto.datasciencecentral.com/autoregressive-model/

The model is given as AR(p) where p is the order of autoregressive part and gives the info about how many lagged observation have to be taken. 
This model can be summarised as a linear regression model whose Equation is given as
y<sub>t</sub><= δ + φ<sub>1</sub>y<sub>t-1</sub> + φ<sub>2</sub>y<sub>t-2</sub> + … + φ<sub>p</sub>y<sub>t-1</sub> + A<sub>t</sub>


Where:
* y<sub>t-1</sub>, y<sub>t-1</sub>, y<sub>t-2</sub>….y<sub>t-p</sub>  are the lagged values.
* A<sub>t</sub> is white noise(randomness)
* δ is given as :

![delta equ](https://www.statisticshowto.datasciencecentral.com/wp-content/uploads/2015/08/autoregressive-model.png) 

where μ  is the process mean.

AR(0): No dependence between the terms. Only the error/noise term contributes to the output of the process.
AR(1): Only the previous term in the process and the noise term contribute to the output.
AR(2): The previous two terms and the noise term contribute to the output.

# Moving Average (MA) Model

A moving average term in a time series model is a past error (multiplied by a coefficient). 
Instead of using past values of the series in a regression, It uses the series mean and previous errors to make predictions.

![equ_ma](https://latex.codecogs.com/gif.latex?y_%7Bt%7D%20%3D%20c%20&plus;%20%5Cvarepsilon_t%20&plus;%20%5Ctheta_%7B1%7D%5Cvarepsilon_%7Bt-1%7D%20&plus;%20%5Ctheta_%7B2%7D%5Cvarepsilon_%7Bt-2%7D%20&plus;%20%5Cdots%20&plus;%20%5Ctheta_%7Bq%7D%5Cvarepsilon_%7Bt-q%7D%2C)

where ε<sub>t</sub> is white noise.  
Here the series current deviation from mean, depends on previous deviations.

The model is given as MA(q) where q is the Order of the moving-average process and gives the  number of lagged forecast errors in the prediction equation.

>MA(q) models are very similar to AR(p) models. The difference is that the MA(q) model is a linear combination of past white noise error terms as opposed to a linear combo of past observations like the AR(p) model. The motivation for the MA model is that we can observe "shocks" in the error process directly by fitting a model to the error terms. In an AR(p) model these shocks are observed indirectly by using the ACF on the series of past observations.


